\documentclass{meta}
\usepackage{amsmath,amssymb}

\usepackage{graphicx}

\usepackage{tikz,tkz-tab}

\def\N{{\mathcal{N}}}
\def\NN{{\mathbb{N}}}
\def\ZZ{{\mathbb{Z}}}
\def\RR{{\mathbb{R}}}
\def\un{{\rm{1\!\!1}}}
\def\CC{{\mathcal{C}}}
\def\HH{{\mathcal{H}}}
\def\UU{{\mathcal{U}}}
\def\II{{\mathcal{I}}}
\def\JJ{{\mathcal{J}}}
\def\TT{{\mathcal{T}}}
\def\PP{{\mathcal{P}}}
\def\SS{{\mathcal{S}}}
\def\WW{{\mathcal{W}}}
\def\KK{{\mathcal{K}}}
\def\MM{{\mathcal{M}}}

\newtheorem{mydef}{Definition}

 \newtheorem{prop}{Proposition}
 \newtheorem{conj}{Conjecture}
 


\begin{document}


\pagestyle{headings}

\mainmatter

\title{TER - Clustering Algorithms in a 2-dimensional Pareto Front}

% Title
\titlerunning{TER - Clustering Algorithms in a 2-dimensional Pareto Front}

% Title for odd pages
\author{ HE Yuru and XU Yuetian }

% Authors for the top of the even pages
\authorrunning{HE Yuru ,XU Yuetian}

\institute{Universit√© Paris-Sud, Informatique Master1 \\
\email{yuru.he@u-psud.fr,yuetian.xu@u-psud.fr}}

\maketitle

%\maketitle
\begin{abstract}
This paper is motivated by a real life application of multi-objective optimization without preference.
Having many incomparable solutions with Pareto optimality, 
the motivation is to select a small number of representative solutions for decision makers.This paper proves that these clustering problems can be solved to optimality with a unified dynamic programming algorithm. 
The clustering measures is investigated in this paper for the $2$-dimensional case
using the specific property that the points to cluster are Pareto optimal in $\RR^2$.

\end{abstract}

\noindent{\textbf{Keywords}:
Clustering algorithms; Pareto frontier; 
dynamic programming; matheuristics }


\section{Problem Description}

Clustering is a statistical analysis method used to organize raw data into homogeneous silos. Within each cluster, the data is grouped according to a common characteristic. The scheduling tool is an algorithm that measures the proximity between each element based on defined criteria. 

In this paper, the representativity measure comes from clustering algorithms, partitioning the N elements into K subsets with a maximal similarity, and giving a representative (i.e. central) element of the optimal clusters.

There are five clustering measures mentioned in this paper: $k$-means,  $k$-medoids, $k$-median, discrete $k$-center,continuous  $k$-center.

$k$-means is a simple and effective measure.In each cluster, there is an averaged center called centroids .It clustering minimizes the sum of squared distance from each item to its nearest averaged center.

For $k$-medoids,in each cluster, there is a medoid, which is a real data item from the data set.$k$-medoids clustering minimizes the sum of squared distance from each item to its nearest medoids.

For $k$-median ,in each cluster, there is a median.$k$-median clustering minimizes the sum of distance from each item to its nearest median.

For $k$-center, in each cluster, there is a cluster center.$k$-center clustering minimizes the maximum distance from each item to its nearest cluster centers.

%Industrial applications of Pareto Front in multi-objective optimization: system engineering, design of industrial systems.In the case of dimension 2, minimizing 2 objectives, we can define Pareto fronts more easily.

%The previous problems are NP-hard in general in  $\RR^2$.
%Being in a Pareto Front, it implies a specilic property:
%Property (Interval clustering is optimal) For the previous clustering problems, optimal solutions exist using only clusters 
%$\CC_{i,i'} = 
%\{x_j\}_{j \in \![i,i']\!]}= \{x \in E \: | \: \exists j \in \![i,i']\!],\: x = x_j \} $.



\section{Notations}

%In this section, we define the properties of a Pareto Frontier,

%\subsection{Pareto frontier}

%Pareto front propriety, non dominated points



We suppose in this paper having a set $E=\{x_1,\dots, x_N\}$ of $N$ elements of $\RR^2$, %fulfilling hypothesis (\ref{hypoNonDominated})
such that for all $ i\neq j$, $x_i \phantom{0} \mathcal{I} \phantom{0} x_j$  
% \begin{equation}
% \forall i\neq j, \hskip 1cm x_i \mathcal{I} x_j \label{hypoNonDominated}
% \end{equation}
defining the binary relations $\mathcal{I},\prec $  for all $ y=(y^1,y^2),z=(z^1,z^2) \in \RR^2$ with:
\begin{eqnarray}
y \phantom{1}\mathcal{I}\phantom{1} z  & \Longleftrightarrow  &y \prec z  \phantom{2} \mbox{or} \phantom{2}  z \prec y \\
 y \prec z  & \Longleftrightarrow  & y^1< z^1 \phantom{2} \mbox{and}\phantom{2} y^2> z^2
\end{eqnarray}


This property  is verified in the applicative context, $E$ being the solution of a bi-objective optimization problem without preference.
This applies for 
exact approaches or population meta-heuristics like evolutionary algorithms and others \cite{talbi2012multi}.



%We define following distance between points of $E$, 
We consider in this paper the Euclidian distance, defining for all $ y=(y^1,y^2),z=(z^1,z^2) \in \RR^2$: 
\begin{equation}
%\forall y=(y^1,y^2),z=(z^1,z^2) \in \RR^2, \phantom{2}  
d(y,z) = |\!| y -z |\!| = \sqrt{ \left(y^1 - z^1\right)^2 + \left(y^2 - z^2\right)^2}
\end{equation}



We define  $\Pi_K(E)$, as the set of all the possible partitions of $E$ in $K$ subsets:
\begin{equation}
\Pi_K(E) = \left\{P \subset \PP(E)\: \bigg| \:\forall p,p' \in P, \:\:p \cap p' =  \emptyset \: \mbox{and} \: \bigcup_{p \in P} = E \: \mbox{and} \; \mbox{card}(P)=K \: \right\} 
\end{equation}

Defining a cost function $f$ for each subset of E to measure the dissimilarity, the clustering problem is written as following optimization problem:
\begin{equation}\label{defK}
\min_{\pi \in \Pi_K(E)}  
\sum_{p \in \pi}  f(P)
\end{equation}

K-means clustering  is a combinatorial optimization problems indexed by $\Pi_K(E)$.
K-means clustering minimizes the sum for all the $K$ clusters of the average distances from the points of the clusters to the centroid.
Mathematically, this can be written as:
\begin{equation}\label{defKmeans}
f_{means}(P) =  \frac 1 {\mbox{card}(p)}
 \sum_{x \in p} \left|\!\left| x - \frac 1 {\mbox{card}(p)} \sum_{y \in p} y \right|\!\right|^2
\end{equation}

K-medoids clustering minimizes the sum of squared distance from each item to its nearest medoids.Mathematically, this can be written as:
\begin{equation}\label{defKedoids}
f_{medoids}(P) =  \frac 1 {\mbox{card}(p)}\min_{y \in p} 
\sum_{x \in p} \left|\!\left| x - y \right|\!\right|^2
\end{equation}

K-median clustering minimizes the sum of distance from each item to its nearest median.Mathematically, this can be written as:
\begin{equation}\label{defKmeidan}
f_{median}(P) = \min_{y \in p} 
\sum_{x \in p} \left|\!\left| x - y \right|\!\right|
\end{equation}

Discrete K-center can be written as:
\begin{equation}\label{defKcenterD}
f_{ctr}^D (P) = \min_{y \in p} 
\max_{x \in p} \left|\!\left| x - y \right|\!\right|
\end{equation}

Continous K-center can be writtern as:
\begin{equation}\label{defKcenterC}
f_{ctr}^C (P) = \min_{y \in \RR^2 } 
\max_{x \in p} \left|\!\left| x - y \right|\!\right|
\end{equation}


Industrial applications of Pareto Front in multi-objective optimization: system engineering, design of industrial systems.
In the case of dimension 2, minimizing 2 objectives, we can define Pareto fronts more easily:

A discrete 2-dimensional Pareto Front is defined here as a set E of N points in $\RR^2$ , indexed with $E = {(x_k,y_k)}_{k \in [\![1,N]\!]}$
such that 
$k \in [\![1,N]\!] \mapsto x_k$ is increasing and $k \in [\![1,N]\!] \mapsto y_k$ is decreasing.



  \begin{figure}[htbp]
      \centering 
   \begin{tikzpicture}[scale=0.5]
    %\draw[very thin,color=gray] (-1,-1) grid (1,1);
    \draw[->] (0,0) -- (20,0) node[right] {$\emph y$};
    \draw[->] (0,0) -- (0,10.6) node[above] {$\emph x$};
    \draw (1.2,9.7) node[above] {$z_1$};
    \draw (1,9.7) node[color=blue!50] {$\bullet$};
    \draw (1.4,8.1) node[above] {$z_2$};
    \draw (1.7,8.1) node[color=blue!50] {$\bullet$};
    \draw (2.7,6.8) node[above] {$z_3$};
    \draw (2.5,6.8) node[color=blue!50] {$\bullet$};
    \draw (3.7,6.4) node[above] {$z_4$};
    \draw (3.5,6.4) node[color=blue!50] {$\bullet$};
    \draw (4.7,6.1) node[above] {$z_5$};
    \draw (4.5,6.1) node[color=blue!50] {$\bullet$};
    \draw (5.2,5.3) node[above] {$z_6$};
    \draw (5,5.3) node[color=blue!50] {$\bullet$}; 
    \draw (6.5,5.1) node[above] {$z_7$};
    \draw (6.3,5.1) node[color=blue!50] {$\bullet$};
    \draw (8.2,4.7) node[above] {$z_8$};
    \draw (8,4.7) node[color=blue!50] {$\bullet$};
    \draw (9.2,3.4) node[above] {$z_9$};
    \draw (9,3.4) node[color=blue!50] {$\bullet$};
    \draw (10.2,3.1) node[above] {$z_{10}$};
    \draw (10,3.1) node[color=blue!50] {$\bullet$};
    \draw (11.2,2.7) node[above] {$z_{11}$};
    \draw (11,2.7) node[color=blue!50] {$\bullet$};
    \draw (12.9,2.4) node[above] {$z_{12}$};
    \draw (12.7,2.4) node[color=blue!50] {$\bullet$};
    \draw (14.7,2.1) node[above] {$z_{13}$};
    \draw (14.3,2.1) node[color=blue!50] {$\bullet$};
    \draw (16.6,1.4) node[above] {$z_{14}$};
    \draw (16.4,1.4) node[color=blue!50] {$\bullet$};
    \draw (18.9,0.9) node[above] {$z_{15}$};
    \draw (18.7,0.9) node[color=blue!50] {$\bullet$};
    \end{tikzpicture}
    \caption{Illustration of  the indexation implied by Proposition in a 2-d Pareto front }\label{orderIllustr}  
   \end{figure}





\section{Dynamic Programming Algorithm}

%Algo valid and common for the 4 clustering algorithms once  having computed $c_{i,i'}$ with $i<i'$, 


Conjecture and the polynomial computations of all the $c_{i,i'}$ with $i<i'$
allows to derive a dynamic programming algorithm.
Defining $C_{i,k}$ as the optimal cost of the $k$-means clustering with $k$ cluster among points $[\![1,i]\!]$ for all $i \in [\![1,N]\!]$ and $k \in [\![1,K]\!]$,
we have following induction relation:

\begin{equation}\label{inducForm}
\forall i \in [\![1,N]\!], \: \forall k \in [\![2,K]\!], \:\:\: C_{i,k} = \min_{j \in [\![1,i]\!]} C_{j-1,k-1} + c_{j,i}
\end{equation}
This last relation use the convention that $C_{0,k} = 0$ for all $k\geqslant0$.
These relations allow to compute the optimal values of $C_{i,k}$ for all $i \in [\![1,N]\!]$ and  $k \in [\![1,K]\!]$. The optimal partition is computed with backtracking.

\begin{figure}[ht]
 \centering 
\begin{tabular}{ l }
\hline
\textbf{Algorithm 1: DP interval clustering in a 2d-Pareto Front }\\
\hline
\verb!  !\\

compute cost matrix $c_{i,j}$ for all $(i,j) \in   [\![1,N]\!]^2$\\


\verb!  !\\

\verb!  ! \textbf{for} $i=1$ to $N$  //Construction of the matrix $C$\\
\verb!    ! set $C_{i,k} = c_{1,i}$  // case $k=1$ treated separately\\
\verb!    ! \textbf{for} $k=2$ to $K$ \\
\verb!      ! set $C_{i,k} = \min_{j \in [\![1,i]\!]} C_{j-1,k-1} + c_{j,i}$\\
\verb!    ! \textbf{end for} \\
\verb!  ! \textbf{end for} \\
\textbf{return} $C_{N,K}$ the optimal cost \\

\verb!  !\\

\verb!  !     initialize $i=N$ and $P=nil$  //Backtrack phase\\
\verb!  ! \textbf{for} $k=K$ to $1$ with increment $k \leftarrow k-1$\\
\verb!    ! find $j\in [\![1,i]\!]$ such that $C_{i,k} = C_{j-1,k-1} + c_{j,i}$\\
\verb!    ! add $[\![j,i]\!]$ in $\PP$\\
\verb!    ! %\textbf{if} $j>1$ \textbf{then}
$i=j-1$\\
\verb!  ! \textbf{end for} \\

\verb!  !\\

\textbf{return} the partition $\PP$ giving the cost $C_{N,K}$\\
\hline
\end{tabular}
\end{figure}

\section{ Lloyd Algorithm }








\end{document}
